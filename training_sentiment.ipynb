{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTEw5lbndcAF"
      },
      "source": [
        "# Clear Output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oqaJLBWIddQs"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qarfl536TLhw"
      },
      "source": [
        "# Installing Transformers and Datasets from HuggingFace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EtKp3PwRyR5Y"
      },
      "outputs": [],
      "source": [
        "! pip install transformers datasets\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWq2Uk-iTOia"
      },
      "source": [
        "# Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "F2hqwP1hxvc-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import DistilBertTokenizerFast,DistilBertForSequenceClassification, AutoTokenizer, AutoModelForTokenClassification, AutoModelForSequenceClassification\n",
        "from transformers import Trainer,TrainingArguments, pipeline\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "16htDkBZ0b2O"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('news_data_ROW 1 - 1330.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "df2 = df[['title', 'Sentimental Analysis' ]].dropna().rename(columns = {'title': 'text', 'Sentimental Analysis': 'score'})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObDWo4TzTh74"
      },
      "source": [
        "# Re-scale labelled sentiment to 0,1,2 which represents negative, neutral and positive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Mh-ogmh3buGR"
      },
      "outputs": [],
      "source": [
        "def change(x):\n",
        "    if x==0:\n",
        "        return 1\n",
        "    if x==-1:\n",
        "        return 0\n",
        "    if x==1:\n",
        "        return 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nhvAi-HT3aQ"
      },
      "source": [
        "Manually labelled dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "1AdAyD6yb4Wx",
        "outputId": "fc46d61e-8046-4189-e6c1-b3e6b591d156"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Cloverleaf Networks Acquires Ryver to Enhance ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Ingalls Shipbuilding Successfully Completes Ac...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>STONERIDGE, INC. TO BROADCAST ITS THIRD-QUARTE...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Volatile Brazil Is Lone Bull Case for Bruised ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Civitas Resources (CIVI) Gains But Lags Market...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  score\n",
              "0  Cloverleaf Networks Acquires Ryver to Enhance ...      1\n",
              "1  Ingalls Shipbuilding Successfully Completes Ac...      1\n",
              "2  STONERIDGE, INC. TO BROADCAST ITS THIRD-QUARTE...      1\n",
              "3  Volatile Brazil Is Lone Bull Case for Bruised ...      2\n",
              "4  Civitas Resources (CIVI) Gains But Lags Market...      2"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df2['score'] = df2.score.apply(change)\n",
        "df2.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpkrrRX1T-KQ"
      },
      "source": [
        "Split to Train-Test set just for manual dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "AINY1i5ab8SZ",
        "outputId": "4cb488c3-a16e-4eea-f0c2-ddd86ee3e828"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df2.text, df2.score, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3IhIeiU6IA6"
      },
      "source": [
        "# Labelled Financial News from Huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset('financial_phrasebank', 'sentences_50agree')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "pdf = pd.DataFrame(dataset['train'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>According to Gran , the company has no plans t...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Technopolis plans to develop in stages an area...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The international electronic industry company ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>With the new production plant the company woul...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>According to the company 's updated strategy f...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4841</th>\n",
              "      <td>LONDON MarketWatch -- Share prices ended lower...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4842</th>\n",
              "      <td>Rinkuskiai 's beer sales fell by 6.5 per cent ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4843</th>\n",
              "      <td>Operating profit fell to EUR 35.4 mn from EUR ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4844</th>\n",
              "      <td>Net sales of the Paper segment decreased to EU...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4845</th>\n",
              "      <td>Sales in Finland decreased by 10.5 % in Januar...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4846 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               sentence  label\n",
              "0     According to Gran , the company has no plans t...      1\n",
              "1     Technopolis plans to develop in stages an area...      1\n",
              "2     The international electronic industry company ...      0\n",
              "3     With the new production plant the company woul...      2\n",
              "4     According to the company 's updated strategy f...      2\n",
              "...                                                 ...    ...\n",
              "4841  LONDON MarketWatch -- Share prices ended lower...      0\n",
              "4842  Rinkuskiai 's beer sales fell by 6.5 per cent ...      1\n",
              "4843  Operating profit fell to EUR 35.4 mn from EUR ...      0\n",
              "4844  Net sales of the Paper segment decreased to EU...      0\n",
              "4845  Sales in Finland decreased by 10.5 % in Januar...      0\n",
              "\n",
              "[4846 rows x 2 columns]"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htdbC2htWIK0"
      },
      "source": [
        "Append the manual labelled dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "WyLN557H9-mS",
        "outputId": "592e5da3-a359-4500-9293-6e8c16abd032"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/b1/0fd1b6hs7lz0fm_mh346lybm0000gn/T/ipykernel_90568/1819253798.py:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df3 = df2[['text','score']].append(pdf.rename(columns = {'sentence':'text', 'label':'score'}))\n"
          ]
        }
      ],
      "source": [
        "df3 = df2[['text','score']].append(pdf.rename(columns = {'sentence':'text', 'label':'score'}))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Cloverleaf Networks Acquires Ryver to Enhance ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Ingalls Shipbuilding Successfully Completes Ac...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>STONERIDGE, INC. TO BROADCAST ITS THIRD-QUARTE...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Volatile Brazil Is Lone Bull Case for Bruised ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Civitas Resources (CIVI) Gains But Lags Market...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4841</th>\n",
              "      <td>LONDON MarketWatch -- Share prices ended lower...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4842</th>\n",
              "      <td>Rinkuskiai 's beer sales fell by 6.5 per cent ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4843</th>\n",
              "      <td>Operating profit fell to EUR 35.4 mn from EUR ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4844</th>\n",
              "      <td>Net sales of the Paper segment decreased to EU...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4845</th>\n",
              "      <td>Sales in Finland decreased by 10.5 % in Januar...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6175 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   text  score\n",
              "0     Cloverleaf Networks Acquires Ryver to Enhance ...      1\n",
              "1     Ingalls Shipbuilding Successfully Completes Ac...      1\n",
              "2     STONERIDGE, INC. TO BROADCAST ITS THIRD-QUARTE...      1\n",
              "3     Volatile Brazil Is Lone Bull Case for Bruised ...      2\n",
              "4     Civitas Resources (CIVI) Gains But Lags Market...      2\n",
              "...                                                 ...    ...\n",
              "4841  LONDON MarketWatch -- Share prices ended lower...      0\n",
              "4842  Rinkuskiai 's beer sales fell by 6.5 per cent ...      1\n",
              "4843  Operating profit fell to EUR 35.4 mn from EUR ...      0\n",
              "4844  Net sales of the Paper segment decreased to EU...      0\n",
              "4845  Sales in Finland decreased by 10.5 % in Januar...      0\n",
              "\n",
              "[6175 rows x 2 columns]"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bza2jwSgWK84"
      },
      "source": [
        "Split into train-test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "lQXdwx-m97Zy"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df3.text, df3.score, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zed_X8OZP754"
      },
      "source": [
        "# Sentiment-Analysis "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2F0WxBxZTtm"
      },
      "source": [
        "Name of our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "XDy9LnSSNCWB"
      },
      "outputs": [],
      "source": [
        "model_name = 'distilbert-base-uncased'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eG5mYMoZZcM"
      },
      "source": [
        "Loading Tokenizer from pretrained DistilBERT and define number of label and the droupout rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ft29ffFHNH77",
        "outputId": "55103d3d-64ff-4864-f404-0feb88cc617d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cf3b42090838473c96430274b646be71",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d090c5362c9e449cb0b4f93f4c04455a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "462768c631f54214b569b4f3825d2766",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "441e80665943409893fc66f37b0e3709",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/483 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model_name = 'distilbert-base-uncased'\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased',num_labels=3, dropout=0.5, attention_dropout=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7AuezrAZiNQ"
      },
      "source": [
        "Encoding the input datatset to vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "CeyosUjfNKL7"
      },
      "outputs": [],
      "source": [
        "model_name = 'distilbert-base-uncased'\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased',num_labels=3, dropout=0.5, attention_dropout=0.5)\n",
        "train_encodings = tokenizer(X_train.to_list(), truncation=True, padding=True,return_tensors = 'pt')\n",
        "val_encodings = tokenizer(X_test.to_list(), truncation=True, padding=True,return_tensors = 'pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NldJBUdZnn3"
      },
      "source": [
        "Defining class Dataset which PyTorch requires"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "uvkn9ptsNobi"
      },
      "outputs": [],
      "source": [
        "class SentimentDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "kv2mPDGWNyNk"
      },
      "outputs": [],
      "source": [
        "train_dataset = SentimentDataset(train_encodings, y_train.to_list())\n",
        "val_dataset = SentimentDataset(val_encodings, y_test.to_list())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euWZZNlIZszZ"
      },
      "source": [
        "Hyper-parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_ftCOe8N1sd",
        "outputId": "4920b7a9-611a-4a0b-9f32-431fb9daa7ee"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "using `logging_steps` to initialize `eval_steps` to 50\n",
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
          ]
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='training',          # output directory\n",
        "    evaluation_strategy=\"steps\",\n",
        "    num_train_epochs=10,              # total number of training epochs\n",
        "    per_device_train_batch_size=32,  # batch size per device during training\n",
        "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs4',            # directory for storing logs\n",
        "    logging_steps=50,\n",
        "    load_best_model_at_end=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fN9NFWsZ1dY"
      },
      "source": [
        "Define a custom computer metrics to get Accuracy and f1_score during the training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "GBNXbxX0N0ZB"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "def compute_metrics(p):\n",
        "    pred, labels = p\n",
        "    pred = np.argmax(pred, axis=1)\n",
        "\n",
        "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
        "    #recall = recall_score(y_true=labels, y_pred=pred)\n",
        "    #precision = precision_score(y_true=labels, y_pred=pred)\n",
        "    f1 = f1_score(labels, pred, average='weighted')\n",
        "\n",
        "    return {\"accuracy\": accuracy,\"f1_score\":f1}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rI1Egq54Z6fh"
      },
      "source": [
        "Instantiate the model and start training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5qrMs3T5N33g",
        "outputId": "ff709f79-0ec0-4887-c9e3-c0b4bf82d969"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file config.json from cache at /Users/bytedance/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/043235d6088ecd3dd5fb5ca3592b6913fd516027/config.json\n",
            "Model config DistilBertConfig {\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.5,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.5,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.23.1\",\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /Users/bytedance/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/043235d6088ecd3dd5fb5ca3592b6913fd516027/pytorch_model.bin\n",
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias']\n",
            "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/Users/bytedance/Library/Python/3.9/lib/python/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 4940\n",
            "  Num Epochs = 10\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 1550\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3daf7fc2b75e471bb28b182ec40c77c6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1550 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/b1/0fd1b6hs7lz0fm_mh346lybm0000gn/T/ipykernel_90568/283397799.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1235\n",
            "  Batch size = 64\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.0616, 'learning_rate': 5e-06, 'epoch': 0.32}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "11b43cc44c49413693f037bf2fa60a76",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 1.0090727806091309, 'eval_accuracy': 0.6097165991902834, 'eval_f1_score': 0.4618879267507881, 'eval_runtime': 19.9103, 'eval_samples_per_second': 62.028, 'eval_steps_per_second': 1.005, 'epoch': 0.32}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/b1/0fd1b6hs7lz0fm_mh346lybm0000gn/T/ipykernel_90568/283397799.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1235\n",
            "  Batch size = 64\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.9324, 'learning_rate': 1e-05, 'epoch': 0.65}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c33e1237158f440cb08f2e16c9a5ed60",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.9025490283966064, 'eval_accuracy': 0.6097165991902834, 'eval_f1_score': 0.4618879267507881, 'eval_runtime': 19.7808, 'eval_samples_per_second': 62.434, 'eval_steps_per_second': 1.011, 'epoch': 0.65}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/b1/0fd1b6hs7lz0fm_mh346lybm0000gn/T/ipykernel_90568/283397799.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1235\n",
            "  Batch size = 64\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.8563, 'learning_rate': 1.5e-05, 'epoch': 0.97}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8a03000338f74e77b226d4727d325560",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.8422989845275879, 'eval_accuracy': 0.6178137651821862, 'eval_f1_score': 0.4823997529226821, 'eval_runtime': 18.636, 'eval_samples_per_second': 66.269, 'eval_steps_per_second': 1.073, 'epoch': 0.97}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/b1/0fd1b6hs7lz0fm_mh346lybm0000gn/T/ipykernel_90568/283397799.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1235\n",
            "  Batch size = 64\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.8178, 'learning_rate': 2e-05, 'epoch': 1.29}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3e8da498836c4d6fbdfad7c8f810d948",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.775526225566864, 'eval_accuracy': 0.6615384615384615, 'eval_f1_score': 0.5784195464857611, 'eval_runtime': 19.5106, 'eval_samples_per_second': 63.299, 'eval_steps_per_second': 1.025, 'epoch': 1.29}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/b1/0fd1b6hs7lz0fm_mh346lybm0000gn/T/ipykernel_90568/283397799.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1235\n",
            "  Batch size = 64\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.7404, 'learning_rate': 2.5e-05, 'epoch': 1.61}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b4d1337eb68c42bca9ab62dbf935b13b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.7193660140037537, 'eval_accuracy': 0.668825910931174, 'eval_f1_score': 0.5955975960067385, 'eval_runtime': 19.8569, 'eval_samples_per_second': 62.195, 'eval_steps_per_second': 1.007, 'epoch': 1.61}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/b1/0fd1b6hs7lz0fm_mh346lybm0000gn/T/ipykernel_90568/283397799.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1235\n",
            "  Batch size = 64\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.6543, 'learning_rate': 3e-05, 'epoch': 1.94}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fa3d9e8c4c60454882629c4e77ae155f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.6287842988967896, 'eval_accuracy': 0.7327935222672065, 'eval_f1_score': 0.6917034048059206, 'eval_runtime': 19.817, 'eval_samples_per_second': 62.32, 'eval_steps_per_second': 1.009, 'epoch': 1.94}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/b1/0fd1b6hs7lz0fm_mh346lybm0000gn/T/ipykernel_90568/283397799.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1235\n",
            "  Batch size = 64\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.6174, 'learning_rate': 3.5e-05, 'epoch': 2.26}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8136dc732a8d4887a34d65a6b6610235",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.5734500885009766, 'eval_accuracy': 0.7497975708502024, 'eval_f1_score': 0.7179064613806804, 'eval_runtime': 20.051, 'eval_samples_per_second': 61.593, 'eval_steps_per_second': 0.997, 'epoch': 2.26}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/b1/0fd1b6hs7lz0fm_mh346lybm0000gn/T/ipykernel_90568/283397799.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1235\n",
            "  Batch size = 64\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.5525, 'learning_rate': 4e-05, 'epoch': 2.58}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c41e9263d2ed43bca0326f38589c1534",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.5187347531318665, 'eval_accuracy': 0.7692307692307693, 'eval_f1_score': 0.7551111009267645, 'eval_runtime': 19.1615, 'eval_samples_per_second': 64.452, 'eval_steps_per_second': 1.044, 'epoch': 2.58}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/b1/0fd1b6hs7lz0fm_mh346lybm0000gn/T/ipykernel_90568/283397799.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1235\n",
            "  Batch size = 64\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.5431, 'learning_rate': 4.5e-05, 'epoch': 2.9}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "48bb4093410c4cc9bad965e1d5a81d73",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.5188854336738586, 'eval_accuracy': 0.7781376518218623, 'eval_f1_score': 0.7801954598013459, 'eval_runtime': 19.7679, 'eval_samples_per_second': 62.475, 'eval_steps_per_second': 1.012, 'epoch': 2.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/b1/0fd1b6hs7lz0fm_mh346lybm0000gn/T/ipykernel_90568/283397799.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1235\n",
            "  Batch size = 64\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.4989, 'learning_rate': 5e-05, 'epoch': 3.23}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "003240ac44b14f499b63cf2d8124cb15",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to training/checkpoint-500\n",
            "Configuration saved in training/checkpoint-500/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.5009760856628418, 'eval_accuracy': 0.794331983805668, 'eval_f1_score': 0.7963676282309027, 'eval_runtime': 18.1562, 'eval_samples_per_second': 68.021, 'eval_steps_per_second': 1.102, 'epoch': 3.23}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in training/checkpoint-500/pytorch_model.bin\n",
            "/var/folders/b1/0fd1b6hs7lz0fm_mh346lybm0000gn/T/ipykernel_90568/283397799.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1235\n",
            "  Batch size = 64\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.5204, 'learning_rate': 4.761904761904762e-05, 'epoch': 3.55}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "659b934e93454cb484e99a94e9ea43b3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.4413304924964905, 'eval_accuracy': 0.8178137651821862, 'eval_f1_score': 0.8185034175537553, 'eval_runtime': 17.1498, 'eval_samples_per_second': 72.012, 'eval_steps_per_second': 1.166, 'epoch': 3.55}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/b1/0fd1b6hs7lz0fm_mh346lybm0000gn/T/ipykernel_90568/283397799.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1235\n",
            "  Batch size = 64\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.4736, 'learning_rate': 4.523809523809524e-05, 'epoch': 3.87}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a291a84a3d45496aa0d42e65b158814e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.4664141535758972, 'eval_accuracy': 0.8, 'eval_f1_score': 0.8050266199301466, 'eval_runtime': 17.2963, 'eval_samples_per_second': 71.403, 'eval_steps_per_second': 1.156, 'epoch': 3.87}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/b1/0fd1b6hs7lz0fm_mh346lybm0000gn/T/ipykernel_90568/283397799.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1235\n",
            "  Batch size = 64\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.428, 'learning_rate': 4.2857142857142856e-05, 'epoch': 4.19}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d6324c1edb454606aa24c6967101d2af",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.4425126910209656, 'eval_accuracy': 0.8113360323886639, 'eval_f1_score': 0.8149176568802259, 'eval_runtime': 17.1876, 'eval_samples_per_second': 71.854, 'eval_steps_per_second': 1.164, 'epoch': 4.19}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/b1/0fd1b6hs7lz0fm_mh346lybm0000gn/T/ipykernel_90568/283397799.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1235\n",
            "  Batch size = 64\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.409, 'learning_rate': 4.047619047619048e-05, 'epoch': 4.52}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fd7d0deeb67142d5bfc009815dbd10e8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.40625837445259094, 'eval_accuracy': 0.8412955465587044, 'eval_f1_score': 0.8413864470937807, 'eval_runtime': 17.0693, 'eval_samples_per_second': 72.352, 'eval_steps_per_second': 1.172, 'epoch': 4.52}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/b1/0fd1b6hs7lz0fm_mh346lybm0000gn/T/ipykernel_90568/283397799.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1235\n",
            "  Batch size = 64\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.4358, 'learning_rate': 3.809523809523809e-05, 'epoch': 4.84}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4280a008aa90498f8c4c80300a2c1980",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.44561031460762024, 'eval_accuracy': 0.8113360323886639, 'eval_f1_score': 0.8158938485152638, 'eval_runtime': 17.0924, 'eval_samples_per_second': 72.254, 'eval_steps_per_second': 1.17, 'epoch': 4.84}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/b1/0fd1b6hs7lz0fm_mh346lybm0000gn/T/ipykernel_90568/283397799.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1235\n",
            "  Batch size = 64\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.409, 'learning_rate': 3.571428571428572e-05, 'epoch': 5.16}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6c1a1ebed2914178b6033b52b1aceb84",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.4019330143928528, 'eval_accuracy': 0.8437246963562753, 'eval_f1_score': 0.8448590924085855, 'eval_runtime': 17.0746, 'eval_samples_per_second': 72.329, 'eval_steps_per_second': 1.171, 'epoch': 5.16}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/b1/0fd1b6hs7lz0fm_mh346lybm0000gn/T/ipykernel_90568/283397799.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1235\n",
            "  Batch size = 64\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.3959, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.48}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "85efb92e80f14d539da8df7f93e3e4d0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.3796737790107727, 'eval_accuracy': 0.8647773279352227, 'eval_f1_score': 0.8648234148272979, 'eval_runtime': 17.1169, 'eval_samples_per_second': 72.151, 'eval_steps_per_second': 1.168, 'epoch': 5.48}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/b1/0fd1b6hs7lz0fm_mh346lybm0000gn/T/ipykernel_90568/283397799.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1235\n",
            "  Batch size = 64\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.348, 'learning_rate': 3.095238095238095e-05, 'epoch': 5.81}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "de0c958644174c598bc50da36f7bd9ad",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.4504992663860321, 'eval_accuracy': 0.8161943319838056, 'eval_f1_score': 0.8202364407597721, 'eval_runtime': 17.1451, 'eval_samples_per_second': 72.032, 'eval_steps_per_second': 1.167, 'epoch': 5.81}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/b1/0fd1b6hs7lz0fm_mh346lybm0000gn/T/ipykernel_90568/283397799.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1235\n",
            "  Batch size = 64\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.3498, 'learning_rate': 2.857142857142857e-05, 'epoch': 6.13}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1332b29e08c24d59b332d8a96e7caafe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.4367295205593109, 'eval_accuracy': 0.8299595141700404, 'eval_f1_score': 0.8333816308731581, 'eval_runtime': 17.7073, 'eval_samples_per_second': 69.745, 'eval_steps_per_second': 1.129, 'epoch': 6.13}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/b1/0fd1b6hs7lz0fm_mh346lybm0000gn/T/ipykernel_90568/283397799.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1235\n",
            "  Batch size = 64\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.3314, 'learning_rate': 2.6190476190476192e-05, 'epoch': 6.45}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9b525bf604344bf391df0a103ba493d4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to training/checkpoint-1000\n",
            "Configuration saved in training/checkpoint-1000/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.47549647092819214, 'eval_accuracy': 0.8072874493927126, 'eval_f1_score': 0.8118977019047231, 'eval_runtime': 17.6131, 'eval_samples_per_second': 70.118, 'eval_steps_per_second': 1.136, 'epoch': 6.45}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in training/checkpoint-1000/pytorch_model.bin\n",
            "/var/folders/b1/0fd1b6hs7lz0fm_mh346lybm0000gn/T/ipykernel_90568/283397799.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1235\n",
            "  Batch size = 64\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.3329, 'learning_rate': 2.380952380952381e-05, 'epoch': 6.77}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e3719113b32c4d4691dd1e09748b92c2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.44489964842796326, 'eval_accuracy': 0.8234817813765182, 'eval_f1_score': 0.8274732363533813, 'eval_runtime': 17.8167, 'eval_samples_per_second': 69.317, 'eval_steps_per_second': 1.123, 'epoch': 6.77}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/b1/0fd1b6hs7lz0fm_mh346lybm0000gn/T/ipykernel_90568/283397799.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1235\n",
            "  Batch size = 64\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.3405, 'learning_rate': 2.1428571428571428e-05, 'epoch': 7.1}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a9bccf3a28904a41b06c62b2e33f18e2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.4019692838191986, 'eval_accuracy': 0.8396761133603239, 'eval_f1_score': 0.8420816352775391, 'eval_runtime': 17.7236, 'eval_samples_per_second': 69.681, 'eval_steps_per_second': 1.128, 'epoch': 7.1}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/b1/0fd1b6hs7lz0fm_mh346lybm0000gn/T/ipykernel_90568/283397799.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1235\n",
            "  Batch size = 64\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.3107, 'learning_rate': 1.9047619047619046e-05, 'epoch': 7.42}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "51b01253e88648ab84be5cd6b1908639",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.3741989731788635, 'eval_accuracy': 0.8566801619433199, 'eval_f1_score': 0.8576204369475364, 'eval_runtime': 17.7937, 'eval_samples_per_second': 69.406, 'eval_steps_per_second': 1.124, 'epoch': 7.42}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/b1/0fd1b6hs7lz0fm_mh346lybm0000gn/T/ipykernel_90568/283397799.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1235\n",
            "  Batch size = 64\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.3093, 'learning_rate': 1.6666666666666667e-05, 'epoch': 7.74}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "86666e3597454b47aa281c5493375de6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.4003281593322754, 'eval_accuracy': 0.8429149797570851, 'eval_f1_score': 0.8454625869637503, 'eval_runtime': 17.6444, 'eval_samples_per_second': 69.994, 'eval_steps_per_second': 1.134, 'epoch': 7.74}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/b1/0fd1b6hs7lz0fm_mh346lybm0000gn/T/ipykernel_90568/283397799.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1235\n",
            "  Batch size = 64\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.3159, 'learning_rate': 1.4285714285714285e-05, 'epoch': 8.06}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "59aca4f778954d1b8c816d75c78713d0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.40188390016555786, 'eval_accuracy': 0.8453441295546559, 'eval_f1_score': 0.8473978401605198, 'eval_runtime': 17.3936, 'eval_samples_per_second': 71.003, 'eval_steps_per_second': 1.15, 'epoch': 8.06}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/b1/0fd1b6hs7lz0fm_mh346lybm0000gn/T/ipykernel_90568/283397799.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1235\n",
            "  Batch size = 64\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.259, 'learning_rate': 1.1904761904761905e-05, 'epoch': 8.39}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f3623d5dacb84966982f4b1e1e0d0a7b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.40793827176094055, 'eval_accuracy': 0.8502024291497976, 'eval_f1_score': 0.8517307117261931, 'eval_runtime': 17.3394, 'eval_samples_per_second': 71.225, 'eval_steps_per_second': 1.153, 'epoch': 8.39}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/b1/0fd1b6hs7lz0fm_mh346lybm0000gn/T/ipykernel_90568/283397799.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1235\n",
            "  Batch size = 64\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2853, 'learning_rate': 9.523809523809523e-06, 'epoch': 8.71}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5dda2ed10cda46b58c7f9c2dd845288d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.41443493962287903, 'eval_accuracy': 0.8461538461538461, 'eval_f1_score': 0.8481301828175124, 'eval_runtime': 18.823, 'eval_samples_per_second': 65.611, 'eval_steps_per_second': 1.063, 'epoch': 8.71}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/b1/0fd1b6hs7lz0fm_mh346lybm0000gn/T/ipykernel_90568/283397799.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1235\n",
            "  Batch size = 64\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2514, 'learning_rate': 7.142857142857143e-06, 'epoch': 9.03}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "28be1cba0e534cf0a29f4a8b1609b1e4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.44671985507011414, 'eval_accuracy': 0.8340080971659919, 'eval_f1_score': 0.8372416040217894, 'eval_runtime': 18.6734, 'eval_samples_per_second': 66.137, 'eval_steps_per_second': 1.071, 'epoch': 9.03}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/b1/0fd1b6hs7lz0fm_mh346lybm0000gn/T/ipykernel_90568/283397799.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1235\n",
            "  Batch size = 64\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2457, 'learning_rate': 4.7619047619047615e-06, 'epoch': 9.35}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5e857a2433a5425da17e4cd7ecd7cdd9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.43733078241348267, 'eval_accuracy': 0.8364372469635628, 'eval_f1_score': 0.8393011463697656, 'eval_runtime': 17.5707, 'eval_samples_per_second': 70.287, 'eval_steps_per_second': 1.138, 'epoch': 9.35}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/b1/0fd1b6hs7lz0fm_mh346lybm0000gn/T/ipykernel_90568/283397799.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1235\n",
            "  Batch size = 64\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2636, 'learning_rate': 2.3809523809523808e-06, 'epoch': 9.68}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9bc16ecd82174119a85a1d30db34ac49",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to training/checkpoint-1500\n",
            "Configuration saved in training/checkpoint-1500/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.43212178349494934, 'eval_accuracy': 0.8380566801619433, 'eval_f1_score': 0.840893097318563, 'eval_runtime': 17.1964, 'eval_samples_per_second': 71.818, 'eval_steps_per_second': 1.163, 'epoch': 9.68}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in training/checkpoint-1500/pytorch_model.bin\n",
            "/var/folders/b1/0fd1b6hs7lz0fm_mh346lybm0000gn/T/ipykernel_90568/283397799.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1235\n",
            "  Batch size = 64\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2737, 'learning_rate': 0.0, 'epoch': 10.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3a954f445d894826967ea1494736c4dc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from training/checkpoint-1500 (score: 0.43212178349494934).\n",
            "Saving model checkpoint to training\n",
            "Configuration saved in training/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.4336535632610321, 'eval_accuracy': 0.8380566801619433, 'eval_f1_score': 0.8408586216932001, 'eval_runtime': 17.7335, 'eval_samples_per_second': 69.642, 'eval_steps_per_second': 1.128, 'epoch': 10.0}\n",
            "{'train_runtime': 5585.0892, 'train_samples_per_second': 8.845, 'train_steps_per_second': 0.278, 'train_loss': 0.46979688705936556, 'epoch': 10.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in training/pytorch_model.bin\n"
          ]
        }
      ],
      "source": [
        "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\",num_labels=3, dropout=0.5, attention_dropout=0.5)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,# the instantiated Transformers model to be trained\n",
        "    args=training_args, # training arguments, defined above\n",
        "    train_dataset=train_dataset,# training dataset\n",
        "    eval_dataset=val_dataset , # evaluation dataset\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed1BdwexbOFC"
      },
      "source": [
        "#Evaluation of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "R_GCWpn6BHb7",
        "outputId": "e6caa6e4-27a5-42d7-ebd8-579ef449c295"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1235\n",
            "  Batch size = 64\n",
            "/var/folders/b1/0fd1b6hs7lz0fm_mh346lybm0000gn/T/ipykernel_90568/283397799.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6934dca8891040b7ac4f4cbabd505d1d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 0.43212178349494934,\n",
              " 'eval_accuracy': 0.8380566801619433,\n",
              " 'eval_f1_score': 0.840893097318563,\n",
              " 'eval_runtime': 17.8042,\n",
              " 'eval_samples_per_second': 69.366,\n",
              " 'eval_steps_per_second': 1.123,\n",
              " 'epoch': 10.0}"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMAboU3ebX8n"
      },
      "source": [
        "# Visualiaztion "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "XNBmDzU-LPe8"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open('training/checkpoint-1000/trainer_state.json', 'r') as f:\n",
        "    train_data = json.load(f)['log_history']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "xl0gADZiMJ_5"
      },
      "outputs": [],
      "source": [
        "training_loss = {}\n",
        "validation_loss = {}\n",
        "validation_acc = {}\n",
        "for idx, data in enumerate(train_data):\n",
        "    if idx%2==0:\n",
        "        training_loss[data['step']] = data['loss']\n",
        "    else:\n",
        "        validation_loss[data['step']] = data['eval_loss']\n",
        "        validation_acc[data['step']] = data['eval_accuracy']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "zxECqXfIMydK",
        "outputId": "579a2d73-acd5-4969-dd4b-3950a8dbe4d7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Steps')"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(list(training_loss.keys()), list(training_loss.values()))\n",
        "plt.title(\"Training Loss and Accuracy of DistilBert Model\")\n",
        "plt.xlabel(\"Steps\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "8JlfqBviPNwS",
        "outputId": "29e1ca9b-30f1-437d-f7ec-26c89ef3f9f6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Steps')"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(list(validation_loss.keys()), list(validation_loss.values()))\n",
        "plt.plot(list(validation_acc.keys()), list(validation_acc.values()))\n",
        "plt.title(\"Validation Loss and Accuracy of DistilBert Model\")\n",
        "plt.xlabel('Steps')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULuElgx_cAZh"
      },
      "source": [
        "# Testing the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "P-niD2qnTjNf"
      },
      "outputs": [],
      "source": [
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased',num_labels=3)\n",
        "model= DistilBertForSequenceClassification.from_pretrained('training')\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6E3l9t1WdUA8"
      },
      "source": [
        "Creating a pipeline for the sentimen analysis "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "iwuRPQVcT9vD"
      },
      "outputs": [],
      "source": [
        "sentiment = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_m9vgYyPc3dp"
      },
      "source": [
        "Enter desired sentence to test\n",
        "\n",
        "\n",
        "*   LABEL_0: NEGATIVE\n",
        "*   LABEL_1: NEUTRAL\n",
        "*   LABEL_2: POSITIVE\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9YFAbdwUFQR",
        "outputId": "bf2c7808-41cf-4ccb-928d-db27e58fed5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LABEL_0\n",
            "LABEL_1\n",
            "LABEL_2\n"
          ]
        }
      ],
      "source": [
        "print(sentiment('Samsung\\'s stock went down by 20%')[0]['label'])\n",
        "print(sentiment('Samsung\\'s is a company')[0]['label'])\n",
        "print(sentiment('Samsung\\'s stock went up by 20%')[0]['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-11-02 02:27:18.237 \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /Users/bytedance/Library/Python/3.9/lib/python/site-packages/ipykernel_launcher.py [ARGUMENTS]\n"
          ]
        }
      ],
      "source": [
        "# default_text = \"Samsung has made 200% profit, earning $50M this year.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://10.81.28.9:8501\u001b[0m\n",
            "\u001b[0m\n",
            "2022-11-02 02:28:42.718 Uncaught app exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/lib/python3.10/site-packages/streamlit/runtime/scriptrunner/script_runner.py\", line 562, in _run_script\n",
            "    exec(code, module.__dict__)\n",
            "  File \"/Users/bytedance/Library/Python/3.9/lib/python/site-packages/ipykernel_launcher.py\", line 15, in <module>\n",
            "    from ipykernel import kernelapp as app\n",
            "  File \"/Users/bytedance/Library/Python/3.9/lib/python/site-packages/ipykernel/__init__.py\", line 5, in <module>\n",
            "    from .connect import *  # noqa\n",
            "  File \"/Users/bytedance/Library/Python/3.9/lib/python/site-packages/ipykernel/connect.py\", line 11, in <module>\n",
            "    import jupyter_client\n",
            "  File \"/Users/bytedance/Library/Python/3.9/lib/python/site-packages/jupyter_client/__init__.py\", line 8, in <module>\n",
            "    from .asynchronous import AsyncKernelClient  # noqa\n",
            "  File \"/Users/bytedance/Library/Python/3.9/lib/python/site-packages/jupyter_client/asynchronous/__init__.py\", line 1, in <module>\n",
            "    from .client import AsyncKernelClient  # noqa\n",
            "  File \"/Users/bytedance/Library/Python/3.9/lib/python/site-packages/jupyter_client/asynchronous/client.py\", line 6, in <module>\n",
            "    from jupyter_client.channels import HBChannel\n",
            "  File \"/Users/bytedance/Library/Python/3.9/lib/python/site-packages/jupyter_client/channels.py\", line 12, in <module>\n",
            "    import zmq.asyncio\n",
            "  File \"/Users/bytedance/Library/Python/3.9/lib/python/site-packages/zmq/__init__.py\", line 103, in <module>\n",
            "    from zmq import backend\n",
            "  File \"/Users/bytedance/Library/Python/3.9/lib/python/site-packages/zmq/backend/__init__.py\", line 31, in <module>\n",
            "    raise original_error from None\n",
            "  File \"/Users/bytedance/Library/Python/3.9/lib/python/site-packages/zmq/backend/__init__.py\", line 26, in <module>\n",
            "    _ns = select_backend(first)\n",
            "  File \"/Users/bytedance/Library/Python/3.9/lib/python/site-packages/zmq/backend/select.py\", line 31, in select_backend\n",
            "    mod = import_module(name)\n",
            "  File \"/opt/homebrew/Cellar/python@3.10/3.10.8/Frameworks/Python.framework/Versions/3.10/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "  File \"/Users/bytedance/Library/Python/3.9/lib/python/site-packages/zmq/backend/cython/__init__.py\", line 6, in <module>\n",
            "    from . import (\n",
            "ImportError: cannot import name '_device' from partially initialized module 'zmq.backend.cython' (most likely due to a circular import) (/Users/bytedance/Library/Python/3.9/lib/python/site-packages/zmq/backend/cython/__init__.py)\n",
            "^C\n",
            "\u001b[34m  Stopping...\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!streamlit run /Users/bytedance/Library/Python/3.9/lib/python/site-packages/ipykernel_launcher.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
